{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b777cb45-3d46-487e-8bb2-e38df5854a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/viviensiew/.local/share/virtualenvs/llm-zoomcamp-0DI9tvdQ/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from elasticsearch import Elasticsearch\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d799029-349d-476a-b9be-bd4de2baee91",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_client = Elasticsearch('http://localhost:9200') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84488d8b-0089-4dda-8bcf-5cf63de38f0b",
   "metadata": {},
   "source": [
    "### Q1. Getting the embeddings model\n",
    "First, we will get the embeddings model ```multi-qa-distilbert-cos-v1``` from the Sentence Transformer library\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(model_name)\n",
    "```\n",
    "Create the embedding for this user question:\n",
    "```python\n",
    "user_question = \"I just discovered the course. Can I still join it?\"\n",
    "```\n",
    "What's the first value of the resulting vector?\n",
    "\n",
    "* -0.24\n",
    "* -0.04\n",
    "* 0.07\n",
    "* 0.27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88feda40-4f29-4684-89b9-985a4a2d70e7",
   "metadata": {},
   "source": [
    "### Q1 Answer: 0.07822263"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "299a06b7-63c9-44c8-9fc1-16beb8d6d653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07822263\n"
     ]
    }
   ],
   "source": [
    "model_name = \"multi-qa-distilbert-cos-v1\"\n",
    "\n",
    "embedding_model = SentenceTransformer(model_name)\n",
    "user_question = \"I just discovered the course. Can I still join it?\"\n",
    "\n",
    "v = embedding_model.encode(user_question)\n",
    "print(v[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4918ccc3-7f66-4a9e-95c9-73f9dc6532f3",
   "metadata": {},
   "source": [
    "### Prepare the documents\n",
    "Now we will create the embeddings for the documents.\n",
    "\n",
    "Load the documents with ids that we prepared in the module:\n",
    "```python\n",
    "import requests \n",
    "\n",
    "base_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main'\n",
    "relative_url = '03-vector-search/eval/documents-with-ids.json'\n",
    "docs_url = f'{base_url}/{relative_url}?raw=1'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents = docs_response.json()\n",
    "```\n",
    "We will use only a subset of the questions - the questions for \"machine-learning-zoomcamp\". After filtering, you should have only 375 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "170165ea-66f0-4368-87a7-513519278f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "948\n",
      "{\n",
      "  \"text\": \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  \\u201cOffice Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon\\u2019t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
      "  \"section\": \"General course-related questions\",\n",
      "  \"question\": \"Course - When will the course start?\",\n",
      "  \"course\": \"data-engineering-zoomcamp\",\n",
      "  \"id\": \"c02e79ef\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "base_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main'\n",
    "relative_url = '03-vector-search/eval/documents-with-ids.json'\n",
    "docs_url = f'{base_url}/{relative_url}?raw=1'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents = docs_response.json()\n",
    "\n",
    "print(len(documents))\n",
    "print(json.dumps(documents[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4b70ce-9f0f-4a54-990d-18f75f553a02",
   "metadata": {},
   "source": [
    "Filter the questions for course = \"machine-learning-zoomcamp\". After filtering, verify that there are only 375 documents and the results are filtered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed60bb97-823c-410b-8386-d9a107ba7df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"text\": \"Machine Learning Zoomcamp FAQ\\nThe purpose of this document is to capture frequently asked technical questions.\\nWe did this for our data engineering course and it worked quite well. Check this document for inspiration on how to structure your questions and answers:\\nData Engineering Zoomcamp FAQ\\nIn the course GitHub repository there\\u2019s a link. Here it is: https://airtable.com/shryxwLd0COOEaqXo\\nwork\",\n",
      "  \"section\": \"General course-related questions\",\n",
      "  \"question\": \"How do I sign up?\",\n",
      "  \"course\": \"machine-learning-zoomcamp\",\n",
      "  \"id\": \"0227b872\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "375"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_documents = []\n",
    "course = \"machine-learning-zoomcamp\"\n",
    "for doc in documents:\n",
    "    if course in doc['course']:\n",
    "        ml_documents.append(doc)\n",
    "        \n",
    "print(json.dumps(ml_documents[0], indent=2))\n",
    "len(ml_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950b7e6d-a9db-4786-8af0-438f991a2932",
   "metadata": {},
   "source": [
    "### Q2. Creating the embeddings\n",
    "Now for each document, we will create an embedding for both question and answer fields.\n",
    "\n",
    "We want to put all of them into a single matrix X:\n",
    "\n",
    "* Create a list embeddings\n",
    "* Iterate over each document\n",
    "* qa_text = f'{question} {text}'\n",
    "* compute the embedding for qa_text, append to embeddings\n",
    "* At the end, let X = np.array(embeddings) (import numpy as np)\n",
    "\n",
    "What's the shape of X? (X.shape). Include the parantheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca03e4d3-312f-40c9-b812-0e2d3f2a7d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 375/375 [00:35<00:00, 10.56it/s]\n"
     ]
    }
   ],
   "source": [
    "def embedText(doc):\n",
    "    qa_text = f'{doc['question']} {doc['text']}'\n",
    "    return embedding_model.encode(qa_text)\n",
    "    \n",
    "embeddings = [embedText(doc) for doc in tqdm(ml_documents)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78bb67e-e2da-41d1-ba1b-4818df613309",
   "metadata": {},
   "source": [
    "### Q2 Answer: (375, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4796d65-2613-4e5f-919b-dd85b3ed0a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(375, 768)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(embeddings)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c24ef34-ca20-4190-8026-f0d38740bf7f",
   "metadata": {},
   "source": [
    "### Q3. Search\n",
    "We have the embeddings and the query vector. Now let's compute the cosine similarity between the vector from Q1 (let's call it v) and the matrix from Q2.\n",
    "\n",
    "The vectors returned from the embedding model are already normalized (you can check it by computing a dot product of a vector with itself - it should return 1.0). This means that in order to compute the coside similarity, it's sufficient to multiply the matrix X by the vector v:\n",
    "\n",
    "scores = X.dot(v)\n",
    "What's the highest score in the results?\n",
    "\n",
    "* 65.0\n",
    "* 6.5\n",
    "* 0.65\n",
    "* 0.065"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c572d7-4e1f-4aaa-b87f-093fd52611e4",
   "metadata": {},
   "source": [
    "### Q3 Answer: 0.6506573"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9131040b-2eb7-4f03-9a45-38dbc26fac0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6506573"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = X.dot(v)\n",
    "scores.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1eb058-dbe9-44b9-b75b-2e7c17c459fb",
   "metadata": {},
   "source": [
    "### Vector search\n",
    "We can now compute the similarity between a query vector and all the embeddings.\n",
    "\n",
    "Let's use this to implement our own vector search\n",
    "```python\n",
    "class VectorSearchEngine():\n",
    "    def __init__(self, documents, embeddings):\n",
    "        self.documents = documents\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def search(self, v_query, num_results=10):\n",
    "        scores = self.embeddings.dot(v_query)\n",
    "        idx = np.argsort(-scores)[:num_results]\n",
    "        return [self.documents[i] for i in idx]\n",
    "\n",
    "search_engine = VectorSearchEngine(documents=documents, embeddings=X)\n",
    "search_engine.search(v, num_results=5)\n",
    "```\n",
    "If you don't understand how the ```search``` function work:\n",
    "\n",
    "Ask ChatGTP or any other LLM of your choice to explain the code\n",
    "Check our pre-course workshop about implementing a search engine [here](https://github.com/alexeygrigorev/build-your-own-search-engine)\n",
    "\n",
    "(Note: you can replace ```argsort``` with ```argpartition``` to make it a lot faster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94775e18-143d-4b95-b972-67317b7fcdf4",
   "metadata": {},
   "source": [
    "**VectorSearchEngine** instantiates an object with documents and its embeddings. \n",
    "\n",
    "search() will \n",
    "1. Embeds the query,\n",
    "2. Calculates the dot product scores between embedded query and doc embeddings,\n",
    "3. Sort the scores (highest to lowest) and return the top 10 most relevant results (can be adjusted to return n number of top results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0530524-776f-471e-aa79-3583970c04f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Yes, you can. You won’t be able to submit some of the homeworks, but you can still take part in the course.\\nIn order to get a certificate, you need to submit 2 out of 3 course projects and review 3 peers’ Projects by the deadline. It means that if you join the course at the end of November and manage to work on two projects, you will still be eligible for a certificate.',\n",
       " 'section': 'General course-related questions',\n",
       " 'question': 'The course has already started. Can I still join it?',\n",
       " 'course': 'machine-learning-zoomcamp',\n",
       " 'id': 'ee58a693'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorSearchEngine():\n",
    "    def __init__(self, documents, embeddings):\n",
    "        self.documents = documents\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def search(self, v_query, num_results=10):\n",
    "        scores = self.embeddings.dot(v_query)\n",
    "        idx = np.argsort(-scores)[:num_results]\n",
    "        return [self.documents[i] for i in idx]\n",
    "\n",
    "search_engine = VectorSearchEngine(documents=ml_documents, embeddings=X)\n",
    "results = search_engine.search(v, num_results=5)\n",
    "results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb543817-7845-494b-8642-775bd88b141c",
   "metadata": {},
   "source": [
    "### Q4. Hit-rate for our search engine\n",
    "Let's evaluate the performance of our own search engine. We will use the hitrate metric for evaluation.\n",
    "\n",
    "First, load the ground truth dataset:\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "base_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main'\n",
    "relative_url = '03-vector-search/eval/ground-truth-data.csv'\n",
    "ground_truth_url = f'{base_url}/{relative_url}?raw=1'\n",
    "\n",
    "df_ground_truth = pd.read_csv(ground_truth_url)\n",
    "df_ground_truth = df_ground_truth[df_ground_truth.course == 'machine-learning-zoomcamp']\n",
    "ground_truth = df_ground_truth.to_dict(orient='records')\n",
    "```\n",
    "Now use the code from the module to calculate the hitrate of VectorSearchEngine with num_results=5.\n",
    "\n",
    "What did you get?\n",
    "\n",
    "* 0.93\n",
    "* 0.73\n",
    "* 0.53\n",
    "* 0.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fbc8f8b-aedb-4d8d-bf74-4456ba08044c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main'\n",
    "relative_url = '03-vector-search/eval/ground-truth-data.csv'\n",
    "ground_truth_url = f'{base_url}/{relative_url}?raw=1'\n",
    "\n",
    "df_ground_truth = pd.read_csv(ground_truth_url)\n",
    "df_ground_truth = df_ground_truth[df_ground_truth.course == 'machine-learning-zoomcamp']\n",
    "ground_truth = df_ground_truth.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d705ec5-18d8-4055-92a4-ccfa195fa3c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Where can I sign up for the course?',\n",
       " 'course': 'machine-learning-zoomcamp',\n",
       " 'document': '0227b872'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "487726fc-c7ae-4023-afbd-a731a1544b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_rate(relevance_total):\n",
    "    cnt = 0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        if True in line:\n",
    "            cnt = cnt + 1\n",
    "\n",
    "    return cnt / len(relevance_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d97f939-e69e-4c6e-89bf-761942bd0671",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 1830/1830 [00:53<00:00, 34.21it/s]\n"
     ]
    }
   ],
   "source": [
    "relevance_total = []\n",
    "\n",
    "for q in tqdm(ground_truth):\n",
    "    doc_id = q['document']\n",
    "    vec_query = embedding_model.encode(q['question'])\n",
    "    results = search_engine.search(vec_query, num_results=5)\n",
    "    relevance = [d['id'] == doc_id for d in results]\n",
    "    relevance_total.append(relevance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528242b7-b091-4719-93e5-d36e8dde8619",
   "metadata": {},
   "source": [
    "### Q4 Answer: 0.9398907103825137"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd9a2ff3-7793-4eae-b2f6-cb3b1790b16b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9398907103825137"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hit_rate(relevance_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e50294-2f02-438b-a5c2-7d39189c8db7",
   "metadata": {},
   "source": [
    "### Q5. Indexing with Elasticsearch\n",
    "Now let's index these documents with elasticsearch\n",
    "\n",
    "Create the index with the same settings as in the module (but change the dimensions)\n",
    "Index the embeddings (note: you've already computed them)\n",
    "After indexing, let's perform the search of the same query from Q1.\n",
    "\n",
    "What's the ID of the document with the highest score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e525a92-56b5-4eea-926b-088bf7024584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n",
      "(768,)\n"
     ]
    }
   ],
   "source": [
    "# verify dimensions for embedding using Q1 question and embeddings\n",
    "print(v.shape)\n",
    "print(embeddings[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f36a59ff-75a8-4f20-8c6a-f8d3493484b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'ml-course-questions'})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Code modified from Module 3.3.4 evaluate-vector.ipynb\n",
    "\n",
    "dim = v.shape[0]\n",
    "\n",
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"section\": {\"type\": \"text\"},\n",
    "            \"question\": {\"type\": \"text\"},\n",
    "            \"course\": {\"type\": \"keyword\"},\n",
    "            \"id\": {\"type\": \"keyword\"},\n",
    "            # \"question_vector\": {\n",
    "            #     \"type\": \"dense_vector\",\n",
    "            #     \"dims\": dim,\n",
    "            #     \"index\": True,\n",
    "            #     \"similarity\": \"cosine\"\n",
    "            # },\n",
    "            # \"text_vector\": {\n",
    "            #     \"type\": \"dense_vector\",\n",
    "            #     \"dims\": dim,\n",
    "            #     \"index\": True,\n",
    "            #     \"similarity\": \"cosine\"\n",
    "            # },\n",
    "            \"question_text_vector\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": dim,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "index_name = \"ml-course-questions\"\n",
    "\n",
    "es_client.indices.delete(index=index_name, ignore_unavailable=True)\n",
    "es_client.indices.create(index=index_name, body=index_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "823ed8ee-662d-475a-be00-864803cb6a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 375/375 [00:02<00:00, 182.67it/s]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for doc in tqdm(ml_documents):\n",
    "    doc[\"question_text_vector\"] = embeddings[i]\n",
    "    es_client.index(index=index_name, document=doc)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eadbe4a7-e544-4af4-ac23-76948f39a9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code modified from Module 3.3.4 evaluate-vector.ipynb\n",
    "\n",
    "def elastic_search_knn(field, vector, course):\n",
    "    knn = {\n",
    "        \"field\": field,\n",
    "        \"query_vector\": vector,\n",
    "        \"k\": 5,\n",
    "        \"num_candidates\": 10000,\n",
    "        \"filter\": {\n",
    "            \"term\": {\n",
    "                \"course\": course\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    search_query = {\n",
    "        \"knn\": knn,\n",
    "        \"_source\": [\"text\", \"section\", \"question\", \"course\", \"id\"]\n",
    "    }\n",
    "\n",
    "    es_results = es_client.search(\n",
    "        index=index_name,\n",
    "        body=search_query\n",
    "    )\n",
    "    \n",
    "    result_docs = []\n",
    "    \n",
    "    for hit in es_results['hits']['hits']:\n",
    "        result_docs.append(hit['_source'])\n",
    "\n",
    "    return result_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b250bd4-cc8e-479b-928c-4049fce853be",
   "metadata": {},
   "source": [
    "### Q5 Answer: ee58a693"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c488f77-413b-4886-852e-f2f0167bd9ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ee58a693'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q5_results = elastic_search_knn('question_text_vector', v, course)\n",
    "q5_results[0][\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99412abc-7ce9-4e4a-9ed7-58511419f222",
   "metadata": {},
   "source": [
    "### Q6. Hit-rate for Elasticsearch\n",
    "The search engine we used in Q4 computed the similarity between the query and ALL the vectors in our database. Usually this is not practical, as we may have a lot of data.\n",
    "\n",
    "Elasticsearch uses approximate techniques to make it faster.\n",
    "\n",
    "Let's evaluate how worse the results are when we switch from exact search (as in Q4) to approximate search with Elastic.\n",
    "\n",
    "What's hitrate for our dataset for Elastic?\n",
    "\n",
    "* 0.93\n",
    "* 0.73\n",
    "* 0.53\n",
    "* 0.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2520dd8a-5d2b-4393-9637-bc5adeefb472",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 1830/1830 [00:50<00:00, 36.06it/s]\n"
     ]
    }
   ],
   "source": [
    "es_relevance_total = []\n",
    "\n",
    "for q in tqdm(ground_truth):\n",
    "    doc_id = q['document']\n",
    "    vec_query = embedding_model.encode(q['question'])\n",
    "    results = elastic_search_knn('question_text_vector', vec_query, course)\n",
    "    relevance = [d['id'] == doc_id for d in results]\n",
    "    es_relevance_total.append(relevance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882b4107-3946-4473-9970-e45deeec83fa",
   "metadata": {},
   "source": [
    "### Q6 Answer: 0.9398907103825137"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c26e3e33-7dd7-43d6-932a-0b050c46b744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9398907103825137"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hit_rate(es_relevance_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324caa47-aecc-4b22-81e8-b79b807673ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
