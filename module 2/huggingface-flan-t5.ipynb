{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffceda19-cc81-4b45-95b9-81ff345a3cc8",
   "metadata": {},
   "source": [
    "## Base code for module-2\n",
    "Basically a condensed version of what we learned in Module 1:\n",
    "\n",
    "index document for RAG into a minsearch object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bad2beae-8b1b-4298-87cb-794ffb25aeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -f minsearch.py\n",
    "# !wget https://raw.githubusercontent.com/alexeygrigorev/minsearch/main/minsearch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "847ad213-c5a6-4d9e-9e5a-e806519b7c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minsearch.py already exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if not os.path.isfile('minsearch.py'):\n",
    "    !python -m wget \"https://raw.githubusercontent.com/alexeygrigorev/minsearch/main/minsearch.py\"\n",
    "else:\n",
    "    print(\"minsearch.py already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "639f673c-f6e5-4822-ad0e-20a713041517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import minsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7545ec3f-204b-47ac-9336-d9ce43143378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.Index at 0x7f4f2a10a340>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/documents.json?raw=1'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)\n",
    "\n",
    "index = minsearch.Index(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d46ddd92-c18a-41aa-bbdb-4ecb83b66b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc8e6e95-99a7-453b-b0b1-df756d1b825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd5fb35-124d-4ff9-8c28-8d236d218bd3",
   "metadata": {},
   "source": [
    "## Replace the OpenAI LLM with HuggingFace open-source model Google FLAN-T5 XL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690a3e58-2ec5-4e93-a47d-60f5aecae85a",
   "metadata": {},
   "source": [
    "### Important Note: If you're not running in Saturn Cloud \n",
    "\n",
    "You need to install these libraries:\n",
    "\n",
    "Make sure you use the latest versions\n",
    "\n",
    "```pip install -U transformers accelerate bitsandbytes```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eda73f-199b-456a-99e4-d71316f49cfa",
   "metadata": {},
   "source": [
    "By default, the tokenizers are loaded into a default location specified under env variable HF_HOME, usually it's HF_HOME = /home/\\<your username\\>.\n",
    "\n",
    "However on Saturn Cloud, you may not have enough space in your home directory. To check on how much space, use ```!df -h```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ed9effe-18a4-4086-9e5c-b6ede2b7ad32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\r\n",
      "overlay         100G   36G   65G  36% /\r\n",
      "tmpfs            64M     0   64M   0% /dev\r\n",
      "tmpfs           7.7G     0  7.7G   0% /sys/fs/cgroup\r\n",
      "/dev/nvme0n1p1  100G   36G   65G  36% /run\r\n",
      "tmpfs            14G     0   14G   0% /dev/shm\r\n",
      "/dev/nvme2n1    2.0G  134M  1.8G   7% /home/jovyan\r\n",
      "tmpfs            14G  120K   14G   1% /home/jovyan/.saturn\r\n",
      "tmpfs            14G   12K   14G   1% /run/secrets/kubernetes.io/serviceaccount\r\n",
      "tmpfs           7.7G   12K  7.7G   1% /proc/driver/nvidia\r\n",
      "tmpfs           7.7G  4.0M  7.7G   1% /run/nvidia-persistenced/socket\r\n",
      "tmpfs           7.7G     0  7.7G   0% /proc/acpi\r\n",
      "tmpfs           7.7G     0  7.7G   0% /sys/firmware\r\n"
     ]
    }
   ],
   "source": [
    "!df -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37657db9-bb85-48d1-bcae-8cc55c99cba6",
   "metadata": {},
   "source": [
    "We see that there is more space under the directory \"/run\", so we will switch the HF_HOME env variable to \"/run/cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56a9d27e-3b88-4b5f-b0eb-d8b6b9644beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/run/cache/'\n",
    "# equivalent to this terminal cmd: \"export HF_HOME='/run/cache' \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54f0b5c1-8f5b-4854-8f1a-d9cdc0898dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c8fe19-5faf-4d93-9066-87dbd6c9608a",
   "metadata": {},
   "source": [
    "### Tokenizer and LLM\n",
    "\n",
    "The Tokenizer takes in text and turn it into some representation, and then the representation is fed into the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd3bb14f-6ced-45b4-9c8d-11d900c7ce1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb6370b245e44dee9aba88993f5c4628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b25a220526484c4892ba3d6f07448b4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93b6425cd2cf4ca6a147f3b6a2b13f4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cd5fd0ed083483b90c21fd2672aa974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88c3d8bcb6aa4a1c8245f4304bf405bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d30eb88b5eee4749adadd88b781e46f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/53.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca55979779ec49288e8b7e0ed5192d10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "891efa99554640ff82fb909d50081270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.45G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56164d7e0d89484e96ca1e40897d1316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc44553e2ee84df78f052d6a45357add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d549f8ed47e4173aa077be44bd5d583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pip install accelerate\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d03daf-e01c-474f-b767-15cbf18d7dd5",
   "metadata": {},
   "source": [
    "### Translation task\n",
    "The FLAN-T5-XL model was asked to translate a string of text from English to German.\n",
    "\n",
    "Process flow: \n",
    "1. Input_text is encoded with tokenizer into vector representation.\n",
    "2. Model generates the response (outputs) as a vector representation.\n",
    "3. Outputs are decoded by tokenizer into plain text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58039569-c6c6-425c-b0a1-1615af29871f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[13959,  1566,    12,  2968,    10,   571,   625,    33,    25,    58,\n",
       "             1]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"translate English to German: How old are you?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7ed45f9-7bc2-4fc5-94f7-7d572cc064bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[   0, 2739, 4445,  436,  292,   58,    1]], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model.generate(input_ids)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c251174e-d68d-4d03-8df8-de024c591aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> Wie alt sind Sie?</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce958d0c-f26b-48f8-b073-4ba3152a6a6f",
   "metadata": {},
   "source": [
    "### Putting everything together\n",
    "\n",
    "Replace the original prompt and llm functions with the FLAN-T5-XL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3ddbc85-eb1c-491c-9bc5-2934fb6e2096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_prompt(query, search_results):\n",
    "#     prompt_template = \"\"\"\n",
    "# You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "# Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "# QUESTION: {question}\n",
    "\n",
    "# CONTEXT: \n",
    "# {context}\n",
    "# \"\"\".strip()\n",
    "\n",
    "#     context = \"\"\n",
    "    \n",
    "#     for doc in search_results:\n",
    "#         context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "#     prompt = prompt_template.format(question=query, context=context).strip()\n",
    "#     return prompt\n",
    "\n",
    "# def llm(prompt):\n",
    "#     response = client.chat.completions.create(\n",
    "#         model='gpt-4o',\n",
    "#         messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "#     )\n",
    "    \n",
    "#     return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2b503ca-6596-45c6-af8e-02720bf2b92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt\n",
    "\n",
    "def llm(prompt):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    outputs = model.generate(input_ids, )\n",
    "    result = tokenizer.decode(outputs[0])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f54613-9370-46d8-ac67-0a8946ccb8d8",
   "metadata": {},
   "source": [
    "### Modifying the LLM model\n",
    "Expand the max_length to generate a longer response. Also see below.\n",
    "\n",
    "Explanation of Parameters:\n",
    "\n",
    "- ```max_length```: Set this to a higher value if you want longer responses. For example, ```max_length=300```.\n",
    "- ```num_beams```: Increasing this can lead to more thorough exploration of possible sequences. Typical values are between 5 and 10.\n",
    "- ```do_sample```: Set this to True to use sampling methods. This can produce more diverse responses.\n",
    "- ```temperature```: Lowering this value makes the model more confident and deterministic, while higher values increase diversity. Typical values range from 0.7 to 1.5.\n",
    "- ```top_k``` and ```top_p```: These parameters control nucleus sampling. ```top_k``` limits the sampling pool to the top k tokens, while ```top_p``` uses cumulative probability to cut off the sampling pool. Adjust these based on the desired level of randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2897e256-e1aa-45a3-a509-dbea378fd898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt, generate_params=None):\n",
    "    if generate_params is None:\n",
    "        generate_params = {}\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=generate_params.get(\"max_length\", 100),\n",
    "        num_beams=generate_params.get(\"num_beams\", 5),\n",
    "        do_sample=generate_params.get(\"do_sample\", False),\n",
    "        temperature=generate_params.get(\"temperature\", 1.0),\n",
    "        top_k=generate_params.get(\"top_k\", 50),\n",
    "        top_p=generate_params.get(\"top_p\", 0.95),\n",
    "    )\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5a3cee4-7b2e-400b-9b54-f889ccde12bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Yes, even if you don't register, you're still eligible to submit the homeworks. Be aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(\"I just discovered the course. Can I still join it?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83aff4ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
