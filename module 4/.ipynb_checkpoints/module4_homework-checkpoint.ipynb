{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "683d8fa7-fc30-4acc-9bc6-5b7c9ee3b649",
   "metadata": {},
   "source": [
    "## Homework: Evaluation and Monitoring\n",
    "In this homework, we'll evaluate the quality of our RAG system.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ccc824-906b-4aa9-9dbb-ae53a299890e",
   "metadata": {},
   "source": [
    "### Getting the data\n",
    "Let's start by getting the dataset. We will use the data we generated in the module.\n",
    "\n",
    "In particular, we'll evaluate the quality of our RAG system with [gpt-4o-mini](https://github.com/DataTalksClub/llm-zoomcamp/blob/main/04-monitoring/data/results-gpt4o-mini.csv).\n",
    "\n",
    "Read it:\n",
    "\n",
    "```python\n",
    "url = f'{github_url}?raw=1'\n",
    "df = pd.read_csv(url)\n",
    "```\n",
    "We will use only the first 300 documents:\n",
    "\n",
    "```python\n",
    "df = df.iloc[:300]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6c95c5f-7450-46f8-98ab-6959e4fd427e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge in /Users/viviensiew/.local/share/virtualenvs/llm-zoomcamp-0DI9tvdQ/lib/python3.12/site-packages (1.0.1)\n",
      "Requirement already satisfied: six in /Users/viviensiew/.local/share/virtualenvs/llm-zoomcamp-0DI9tvdQ/lib/python3.12/site-packages (from rouge) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed2916a3-75d8-4745-b9df-a9b11d2eee96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/viviensiew/.local/share/virtualenvs/llm-zoomcamp-0DI9tvdQ/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from elasticsearch import Elasticsearch\n",
    "from tqdm.auto import tqdm\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f0ee822-ffe6-444f-9da1-537abe39dd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "github_url = \"https://github.com/DataTalksClub/llm-zoomcamp/blob/main/04-monitoring/data/results-gpt4o-mini.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e845e458-b49c-445c-8ee2-bcca90afcaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f'{github_url}?raw=1'\n",
    "df = pd.read_csv(url)\n",
    "df = df.iloc[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c8b495-e252-4124-a790-c7ab15a6fb84",
   "metadata": {},
   "source": [
    "### Q1. Getting the embeddings model\n",
    "Now, get the embeddings model ```multi-qa-mpnet-base-dot-v1``` from the Sentence Transformer library\n",
    "\n",
    "Note: this is not the same model as in HW3\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(model_name)\n",
    "```\n",
    "Create the embeddings for the first LLM answer:\n",
    "```python\n",
    "answer_llm = df.iloc[0].answer_llm\n",
    "```\n",
    "What's the first value of the resulting vector?\n",
    "\n",
    "* -0.42\n",
    "* -0.22\n",
    "* -0.02\n",
    "* 0.21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ea4122a-74fb-4dab-83a7-acf883d6b8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 3.0.0.dev0, however, your version is 2.7.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"multi-qa-mpnet-base-dot-v1\"\n",
    "embedding_model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24b063b4-7bc0-4a4f-9779-2312baac5a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_llm = df.iloc[0].answer_llm\n",
    "v = embedding_model.encode(answer_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944f649a-634d-4ab7-b522-bb84341e6b07",
   "metadata": {},
   "source": [
    "### Q1 Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e60e9731-2370-4964-86f2-4db5d2d488cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.42244682"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e9ed28-a8ad-47e2-a79c-eb4b6ec5af45",
   "metadata": {},
   "source": [
    "### Q2. Computing the dot product\n",
    "Now for each answer pair, let's create embeddings and compute dot product between them\n",
    "\n",
    "We will put the results (scores) into the evaluations list\n",
    "\n",
    "What's the 75% percentile of the score?\n",
    "\n",
    "* 21.67\n",
    "* 31.67\n",
    "* 41.67\n",
    "* 51.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cecbc6c-fe4a-4e71-b52d-5162accaa4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['v_answer_llm'] = df['answer_llm'].map(lambda x: embedding_model.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a736cc17-0fba-42ae-a80c-0762ec0a7804",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['v_answer_orig'] = df['answer_orig'].map(lambda x: embedding_model.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d04a5ebd-ad78-409d-83ad-e78e61b17d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['score'] = df.apply(lambda x: x['v_answer_llm'].dot(x['v_answer_orig']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639e7da2-e2cd-42ec-b3e5-45a9866d7322",
   "metadata": {},
   "source": [
    "### Q2 Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4706b72d-b597-4e90-91d9-474ed5f8715f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    300.000000\n",
       "mean      27.495996\n",
       "std        6.384743\n",
       "min        4.547927\n",
       "25%       24.307842\n",
       "50%       28.336858\n",
       "75%       31.674304\n",
       "max       39.476013\n",
       "Name: score, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['score'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5611fd-dd3b-4f80-a66c-4981178e2a60",
   "metadata": {},
   "source": [
    "### Q3. Computing the cosine\n",
    "From Q2, we can see that the results are not within the [0, 1] range. It's because the vectors coming from this model are not normalized.\n",
    "\n",
    "So we need to normalize them.\n",
    "\n",
    "To do it, we\n",
    "\n",
    "- Compute the norm of a vector\n",
    "- Divide each element by this norm\n",
    "\n",
    "So, for vector ```v```, it'll be ```v / ||v||```\n",
    "\n",
    "In numpy, this is how you do it:\n",
    "```python\n",
    "norm = np.sqrt((v * v).sum())\n",
    "v_norm = v / norm\n",
    "```\n",
    "Let's put it into a function and then compute dot product between normalized vectors. This will give us cosine similarity\n",
    "\n",
    "What's the 75% cosine in the scores?\n",
    "\n",
    "* 0.63\n",
    "* 0.73\n",
    "* 0.83\n",
    "* 0.93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ef10ed7-b49e-47e5-9061-0e7e62332c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_v(v):\n",
    "    return v / np.sqrt((v * v).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "137d876a-52cf-4687-be0c-90502d534846",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['v_answer_llm_norm'] = df['v_answer_llm'].apply(normalise_v)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ab6bb2f-4873-4e0f-978a-17528037682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['v_answer_orig_norm'] = df['v_answer_orig'].apply(normalise_v)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a807d70-08c0-47ab-87ac-d4f9334ecaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['score_norm'] = df.apply(lambda x: x['v_answer_llm_norm'].dot(x['v_answer_orig_norm']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8df6fa-9569-4712-9c62-2f45a6f3cbaf",
   "metadata": {},
   "source": [
    "### Q3 Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bebba20d-c1fd-4bc6-898a-a72d35264c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    300.000000\n",
       "mean       0.728392\n",
       "std        0.157755\n",
       "min        0.125357\n",
       "25%        0.651273\n",
       "50%        0.763761\n",
       "75%        0.836235\n",
       "max        0.958796\n",
       "Name: score_norm, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['score_norm'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24a949b-c64c-4191-8843-05c9447f85eb",
   "metadata": {},
   "source": [
    "### Q4. Rouge\n",
    "Now we will explore an alternative metric - the ROUGE score.\n",
    "\n",
    "This is a set of metrics that compares two answers based on the overlap of n-grams, word sequences, and word pairs.\n",
    "\n",
    "It can give a more nuanced view of text similarity than just cosine similarity alone.\n",
    "\n",
    "We don't need to implement it ourselves, there's a python package for it:\n",
    "```bash\n",
    "pip install rouge\n",
    "```\n",
    "(The latest version at the moment of writing is 1.0.1)\n",
    "\n",
    "Let's compute the ROUGE score between the answers at the index 10 of our dataframe (doc_id=5170565b)\n",
    "```python\n",
    "from rouge import Rouge\n",
    "rouge_scorer = Rouge()\n",
    "\n",
    "scores = rouge_scorer.get_scores(r['answer_llm'], r['answer_orig'])[0]\n",
    "```\n",
    "There are three scores: ```rouge-1```, ```rouge-2``` and ```rouge-l```, and precision, recall and F1 score for each.\n",
    "\n",
    "* ```rouge-1``` - the overlap of unigrams,\n",
    "* ```rouge-2``` - bigrams,\n",
    "* ```rouge-l``` - the longest common subsequence\n",
    "What's the F score for rouge-1?\n",
    "\n",
    "* 0.35\n",
    "* 0.45\n",
    "* 0.55\n",
    "* 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06bd429f-49b0-429a-86b4-e8fbee798916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5170565b\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[10].document)\n",
    "r = df.iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6ecea97-f247-4a08-9643-bff93183f6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_scorer = Rouge()\n",
    "\n",
    "scores = rouge_scorer.get_scores(r['answer_llm'], r['answer_orig'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc16ffa-4e63-4d58-aafd-22b67758b1c3",
   "metadata": {},
   "source": [
    "### Q4 Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c71c0766-0db6-4e47-9e07-8eaeab2d712c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45454544954545456"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores['rouge-1']['f']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3a3f21-5402-43a6-9936-1755ccecb141",
   "metadata": {},
   "source": [
    "### Q5. Average rouge score\n",
    "Let's compute the average between rouge-1, rouge-2 and rouge-l for the same record from Q4\n",
    "\n",
    "* 0.35\n",
    "* 0.45\n",
    "* 0.55\n",
    "* 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a6433ba-0c0c-4846-88ea-3b5a7ff411a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_scores = [v2 for _,v in scores.items() for _, v2 in v.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7dd0b6-5317-48f7-8c21-9d54aa76b31b",
   "metadata": {},
   "source": [
    "### Q5 Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1943c356-da29-4588-a949-46e369a1b31b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35490035323368824"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(average_scores)/len(average_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa99f0f-537a-4551-ac8a-696750cfcd16",
   "metadata": {},
   "source": [
    "### Q6. Average rouge score for all the data points\n",
    "Now let's compute the score for all the records\n",
    "```python\n",
    "rouge_1 = scores['rouge-1']['f']\n",
    "rouge_2 = scores['rouge-2']['f']\n",
    "rouge_l = scores['rouge-l']['f']\n",
    "rouge_avg = (rouge_1 + rouge_2 + rouge_l) / 3\n",
    "```\n",
    "And create a dataframe from them\n",
    "\n",
    "What's the average rouge_2 across all the records?\n",
    "\n",
    "* 0.10\n",
    "* 0.20\n",
    "* 0.30\n",
    "* 0.40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89ff34bf-8d34-41af-9631-f10d5c8e2a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rouge_scores(answer_llm, answer_orig):\n",
    "    scores = rouge_scorer.get_scores(answer_llm, answer_orig)[0]\n",
    "    # rouge_1 = scores['rouge-1']['f']\n",
    "    rouge_2 = scores['rouge-2']['f']\n",
    "    # rouge_l = scores['rouge-l']['f']\n",
    "    # rouge_avg = (rouge_1 + rouge_2 + rouge_l) / 3\n",
    "    # return rouge_1, rouge_2, rouge_l, rouge_avg\n",
    "    return rouge_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95c4d7c1-eab7-4c8e-8084-1a613f53705a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['rouge_1'], df['rouge_2'], df['rouge_l'], df['rouge_avg'] = df.apply(lambda x: calc_rouge_scores(x['answer_llm'], x['answer_orig']), axis=1)\n",
    "df['rouge_2'] = df.apply(lambda x: calc_rouge_scores(x['answer_llm'], x['answer_orig']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c3f05d-6c3e-4931-97d3-004d447bccc8",
   "metadata": {},
   "source": [
    "### Q6 Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff12edf2-e9ec-410e-8017-2f89f852861d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20696501983423318"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['rouge_2'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6e04fc-df9e-49e2-a082-5367df261ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
